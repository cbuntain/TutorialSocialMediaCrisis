{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import calendar\n",
    "import codecs\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "import gzip\n",
    "import string\n",
    "import glob\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Crisis Anlytics\n",
    "\n",
    "The following notebook walks us through a number of capabilities or common pieces of functionality one may want when analyzing Twitter following a crisis.\n",
    "We will start by defining information for a set of events for which we have data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crisisInfo = {\n",
    "    \"boston\": {\n",
    "        \"name\": \"Boston Marathon Bombing\",\n",
    "        \"time\": 1366051740, # Timestamp in seconds since 1/1/1970, UTC\n",
    "                            # 15 April 2013, 14:49 EDT -> 18:49 UTC\n",
    "        \"directory\": \"boston\",\n",
    "        \"keywords\": [\"boston\", \"exploision\", \"bomb\", \"marathon\"],\n",
    "        \"box\": { # Bounding box for geographic limits\n",
    "            \"lowerLeftLon\": -124.848974,\n",
    "            \"lowerLeftLat\": 24.396308,\n",
    "            \"upperRightLon\": -66.885444,\n",
    "            \"upperRightLat\": 49.384358,\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"paris_hebdo\": {\n",
    "        \"name\": \"Charlie Hebdo Attack\",\n",
    "        \"time\": 1420626600, # Timestamp in seconds since 1/1/1970, UTC\n",
    "                            # 7 January 2015, 11:30 CET -> 10:30 UTC\n",
    "        \"directory\": \"paris_hebdo\",\n",
    "        \"keywords\": [\"paris\", \"hebdo\"],\n",
    "        \"box\": {\n",
    "            \"lowerLeftLon\": -5.1406,\n",
    "            \"lowerLeftLat\": 41.33374,\n",
    "            \"upperRightLon\": 9.55932,\n",
    "            \"upperRightLat\": 51.089062,\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"nepal\": {\n",
    "        \"name\": \"Nepal Earthquake\",\n",
    "        \"time\": 1429942286, # Timestamp in seconds since 1/1/1970, UTC\n",
    "                            # 25 April 2015, 6:11:26 UTC\n",
    "        \"directory\": \"nepal\",\n",
    "        \"keywords\": [\"nepal\", \"earthquake\", \"quake\", \"nsgs\"],\n",
    "        \"box\": {\n",
    "            \"lowerLeftLon\": 80.0562,\n",
    "            \"lowerLeftLat\": 26.3565,\n",
    "            \"upperRightLon\": 88.1993,\n",
    "            \"upperRightLat\": 30.4330,\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"paris_nov\": {\n",
    "        \"name\": \"Paris November Attacks\",\n",
    "        \"time\": 1447446000, # Timestamp in seconds since 1/1/1970, UTC\n",
    "                            # 13 November 2015, 20:20 UTC to 23:58 UTC\n",
    "        \"directory\": \"paris_nov\",\n",
    "        \"keywords\": [\"paris\", \"shots\", \"explosion\"],\n",
    "        \"box\": {\n",
    "            \"lowerLeftLon\": -5.1406,\n",
    "            \"lowerLeftLat\": 41.33374,\n",
    "            \"upperRightLon\": 9.55932,\n",
    "            \"upperRightLat\": 51.089062,\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"brussels\": {\n",
    "        \"name\": \"Brussels Transit Attacks\",\n",
    "        \"time\": 1458629880, # Timestamp in seconds since 1/1/1970, UTC\n",
    "                            # 22 March 2016, 6:58 UTC to 08:11 UTC\n",
    "        \"directory\": \"brussels\",\n",
    "        \"keywords\": [\"brussels\", \"bomb\", \"belgium\", \"explosion\"],\n",
    "        \"box\": {\n",
    "            \"lowerLeftLon\": 2.54563,\n",
    "            \"lowerLeftLat\": 49.496899,\n",
    "            \"upperRightLon\": 6.40791,\n",
    "            \"upperRightLat\": 51.5050810,\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Your Crisis\n",
    "\n",
    "Since we have several disasters we can look at and don't have time to explore them all, you can pick one and follow along with our analysis on the crisis that interests you.\n",
    "\n",
    "To select the crisis you want, pick from the list printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Available Crisis Names:\")\n",
    "for k in sorted(crisisInfo.keys()):\n",
    "    print (\"\\t\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the name below with your selected crisis\n",
    "selectedCrisis = \"nepal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Topic 3.1: Reading Tweets\n",
    "\n",
    "The first thing we do is read in tweets from a directory of compressed files. Our collection of compressed tweets is in the 00_data directory, so we'll use pattern matching (called \"globbing\") to find all the tweet files in the given directory.\n",
    "\n",
    "Then, for each file, we'll open it, read each line (which is a tweet in JSON form), and build an object out of it. As part of this process, we will extract each tweet's post time and create a map from minute timestamps to the tweets posted during that minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine host-specific location of data\n",
    "tweetDirectory = crisisInfo[selectedCrisis][\"directory\"]\n",
    "tweetGlobPath = os.path.join(\"..\", \"00_data\", tweetDirectory, \"statuses.log.*.gz\")\n",
    "\n",
    "print (\"Reading files from:\", tweetGlobPath)\n",
    "\n",
    "# Dictionary for mapping dates to data\n",
    "frequencyMap = {}\n",
    "\n",
    "# For counting tweets\n",
    "globalTweetCounter = 0\n",
    "\n",
    "# Twitter's time format, for parsing the created_at date\n",
    "timeFormat = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "\n",
    "reader = codecs.getreader(\"utf-8\")\n",
    "\n",
    "for tweetFilePath in glob.glob(tweetGlobPath):\n",
    "    print (\"Reading File:\", tweetFilePath)\n",
    "\n",
    "    for line in gzip.open(tweetFilePath, 'rb'):\n",
    "\n",
    "        # Try to read tweet JSON into object\n",
    "        tweetObj = None\n",
    "        try:\n",
    "            tweetObj = json.loads(reader.decode(line)[0])\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        # Deleted status messages and protected status must be skipped\n",
    "        if ( \"delete\" in tweetObj.keys() or \"status_withheld\" in tweetObj.keys() ):\n",
    "            continue\n",
    "\n",
    "        # Try to extract the time of the tweet\n",
    "        try:\n",
    "            currentTime = datetime.datetime.strptime(tweetObj['created_at'], timeFormat)\n",
    "        except:\n",
    "            print (line)\n",
    "            raise\n",
    "\n",
    "        currentTime = currentTime.replace(second=0)\n",
    "\n",
    "        # Increment tweet count\n",
    "        globalTweetCounter += 1\n",
    "\n",
    "        # If our frequency map already has this time, use it, otherwise add\n",
    "        if ( currentTime in frequencyMap.keys() ):\n",
    "            timeMap = frequencyMap[currentTime]\n",
    "            timeMap[\"count\"] += 1\n",
    "            timeMap[\"list\"].append(tweetObj)\n",
    "        else:\n",
    "            frequencyMap[currentTime] = {\"count\":1, \"list\":[tweetObj]}\n",
    "\n",
    "# Fill in any gaps\n",
    "times = sorted(frequencyMap.keys())\n",
    "firstTime = times[0]\n",
    "lastTime = times[-1]\n",
    "thisTime = firstTime\n",
    "\n",
    "# We want to look at per-minute data, so we fill in any missing minutes\n",
    "timeIntervalStep = datetime.timedelta(0, 60)    # Time step in seconds\n",
    "while ( thisTime <= lastTime ):\n",
    "    if ( thisTime not in frequencyMap.keys() ):\n",
    "        frequencyMap[thisTime] = {\"count\":0, \"list\":[]}\n",
    "        \n",
    "    thisTime = thisTime + timeIntervalStep\n",
    "\n",
    "print (\"Processed Tweet Count:\", globalTweetCounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "# Topic 4: Simple Frequency Analysis\n",
    "\n",
    "In this section, we will cover a few simple analysis techniques to garner some small insights rapidly.\n",
    "\n",
    "- Frequency Graph\n",
    "- Top users\n",
    "- Top hash tags\n",
    "- Top URLs\n",
    "- Top images\n",
    "- Most retweeted tweet\n",
    "- Keyword Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Timeline \n",
    "\n",
    "To build a timeline of Twitter usage, we can simply plot the number of tweets posted per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "crisisMoment = crisisInfo[selectedCrisis][\"time\"]\n",
    "crisisTime = datetime.datetime.utcfromtimestamp(crisisMoment)\n",
    "crisisTime = crisisTime.replace(second=0)\n",
    "print (\"Crisis Time:\", crisisTime)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11, 8.5)\n",
    "\n",
    "plt.title(\"Tweet Frequency\")\n",
    "\n",
    "# Sort the times into an array for future use\n",
    "sortedTimes = sorted(frequencyMap.keys())\n",
    "\n",
    "# What time span do these tweets cover?\n",
    "print (\"Time Frame:\", sortedTimes[0], sortedTimes[-1])\n",
    "\n",
    "# Get a count of tweets per minute\n",
    "postFreqList = [frequencyMap[x][\"count\"] for x in sortedTimes]\n",
    "\n",
    "# We'll have ticks every few minutes (more clutters the graph)\n",
    "smallerXTicks = range(0, len(sortedTimes), 10)\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "# Plot the post frequency\n",
    "yData = [x if x > 0 else 0 for x in postFreqList]\n",
    "ax.plot(range(len(frequencyMap)), yData, color=\"blue\", label=\"Posts\")\n",
    "\n",
    "crisisXCoord = sortedTimes.index(crisisTime)\n",
    "ax.scatter([crisisXCoord], [np.mean(yData)], c=\"r\", marker=\"x\", s=100, label=\"Crisis\")\n",
    "\n",
    "ax.grid(b=True, which=u'major')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Twitter Users\n",
    "\n",
    "Finding good sources of information is really important during crises. \n",
    "On Twitter, the loudest or most prolific users are not necessarily good sources though.\n",
    "We first check who these prolific users are by determining who was tweeting the most during this particular time span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create maps for holding counts and tweets for each user\n",
    "globalUserCounter = {}\n",
    "globalUserMap = {}\n",
    "\n",
    "# Iterate through the time stamps\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    # For each tweet, pull the screen name and add it to the list\n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        user = tweet[\"user\"][\"screen_name\"]\n",
    "        \n",
    "        if ( user not in globalUserCounter ):\n",
    "            globalUserCounter[user] = 1\n",
    "            globalUserMap[user] = [tweet]\n",
    "        else:\n",
    "            globalUserCounter[user] += 1\n",
    "            globalUserMap[user].append(tweet)\n",
    "\n",
    "print (\"Unique Users:\", len(globalUserCounter.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortedUsers = sorted(globalUserCounter, key=globalUserCounter.get, reverse=True)\n",
    "print (\"Top Ten Most Prolific Users:\")\n",
    "for u in sortedUsers[:10]:\n",
    "    print (u, globalUserCounter[u], \n",
    "           \"\\n\\t\", \"Random Tweet:\", globalUserMap[u][0][\"text\"], \"\\n----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these tweets are not relevant to the event at hand.\n",
    "Twitter is a very noisy place.\n",
    "\n",
    "Hashtags, however, are high signal keywords. \n",
    "Maybe the most common hashtags will be more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A map for hashtag counts\n",
    "hashtagCounter = {}\n",
    "\n",
    "# For each minute, pull the list of hashtags and add to the counter\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        hashtagList = tweet[\"entities\"][\"hashtags\"]\n",
    "        \n",
    "        for hashtagObj in hashtagList:\n",
    "            \n",
    "            # We lowercase the hashtag to avoid duplicates (e.g., #MikeBrown vs. #mikebrown)\n",
    "            hashtagString = hashtagObj[\"text\"].lower()\n",
    "            \n",
    "            if ( hashtagString not in hashtagCounter ):\n",
    "                hashtagCounter[hashtagString] = 1\n",
    "            else:\n",
    "                hashtagCounter[hashtagString] += 1\n",
    "\n",
    "print (\"Unique Hashtags:\", len(hashtagCounter.keys()))\n",
    "sortedHashtags = sorted(hashtagCounter, key=hashtagCounter.get, reverse=True)\n",
    "print (\"Top Twenty Hashtags:\")\n",
    "for ht in sortedHashtags[:20]:\n",
    "    print (\"\\t\", \"#\" + ht, hashtagCounter[ht])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with URLs to find the most shared URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A map for hashtag counts\n",
    "urlCounter = {}\n",
    "\n",
    "# For each minute, pull the list of hashtags and add to the counter\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        urlList = tweet[\"entities\"][\"urls\"]\n",
    "        \n",
    "        for url in urlList:\n",
    "            urlStr = url[\"url\"]\n",
    "            \n",
    "            if ( urlStr not in urlCounter ):\n",
    "                urlCounter[urlStr] = 1\n",
    "            else:\n",
    "                urlCounter[urlStr] += 1\n",
    "\n",
    "print (\"Unique URLs:\", len(urlCounter.keys()))\n",
    "sortedUrls = sorted(urlCounter, key=urlCounter.get, reverse=True)\n",
    "print (\"Top Twenty URLs:\")\n",
    "for url in sortedUrls[:20]:\n",
    "    print (\"\\t\", url, urlCounter[url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how each URL is shortened using Twitter's shortener. \n",
    "To get a better idea of the content, we should expand the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Top Expanded URLs:\")\n",
    "for url in sortedUrls[:10]:\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        realUrl = r.url\n",
    "        print (\"\\t\", url, urlCounter[url], \"->\", realUrl)\n",
    "    except:\n",
    "        print (\"\\t\", url, urlCounter[url], \"->\", \"UNKNOWN Failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since URLs and Hashtags are both entities, we can do the same for other entities, like mentions and media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A map for mention counts\n",
    "mentionCounter = {}\n",
    "\n",
    "# For each minute, pull the list of mentions and add to the counter\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        mentions = tweet[\"entities\"][\"user_mentions\"]\n",
    "        \n",
    "        for mention in mentions:\n",
    "            mentionStr = mention[\"screen_name\"]\n",
    "            \n",
    "            if ( mentionStr not in mentionCounter ):\n",
    "                mentionCounter[mentionStr] = 1\n",
    "            else:\n",
    "                mentionCounter[mentionStr] += 1\n",
    "\n",
    "print (\"Unique Mentions:\", len(mentionCounter.keys()))\n",
    "sortedMentions = sorted(mentionCounter, key=mentionCounter.get, reverse=True)\n",
    "print (\"Top Twenty Mentions:\")\n",
    "for mention in sortedMentions[:20]:\n",
    "    print (\"\\t\", mention, mentionCounter[mention])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A map for media counts\n",
    "mediaCounter = {}\n",
    "\n",
    "# For each minute, pull the list of media and add to the counter\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        if ( \"media\" not in tweet[\"entities\"] ):\n",
    "            continue\n",
    "            \n",
    "        mediaList = tweet[\"entities\"][\"media\"]\n",
    "        \n",
    "        for media in mediaList:\n",
    "            mediaStr = media[\"media_url\"]\n",
    "            \n",
    "            if ( mediaStr not in mediaCounter ):\n",
    "                mediaCounter[mediaStr] = 1\n",
    "            else:\n",
    "                mediaCounter[mediaStr] += 1\n",
    "\n",
    "print (\"Unique Media:\", len(mediaCounter.keys()))\n",
    "sortedMedia = sorted(mediaCounter, key=mediaCounter.get, reverse=True)\n",
    "print (\"Top Twenty Media:\")\n",
    "for media in sortedMedia[:20]:\n",
    "    print (\"\\t\", media, mediaCounter[media])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can some data is relevant, both in pictures and in hashtags and URLs. \n",
    "Are the most retweeted retweets also useful? \n",
    "Or are they expressing condolence?\n",
    "Or completely unrelated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A map for media counts\n",
    "tweetRetweetCountMap = {}\n",
    "rtList = []\n",
    "\n",
    "# For each minute, pull the list of hashtags and add to the counter\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        tweetId = tweet[\"id_str\"]\n",
    "        rtCount = tweet[\"retweet_count\"]\n",
    "        \n",
    "        if ( \"retweeted_status\" in tweet ):\n",
    "            tweetId = tweet[\"retweeted_status\"][\"id_str\"]\n",
    "            rtCount = tweet[\"retweeted_status\"][\"retweet_count\"]\n",
    "            \n",
    "        tweetRetweetCountMap[tweetId] = rtCount\n",
    "        rtList.append(rtCount)\n",
    "        \n",
    "sortedRetweets = sorted(tweetRetweetCountMap, key=tweetRetweetCountMap.get, reverse=True)\n",
    "print (\"Top Ten Retweets:\")\n",
    "for tweetId in sortedRetweets[:10]:\n",
    "    thisTweet = None\n",
    "    \n",
    "    for t in reversed(sortedTimes):\n",
    "        for tweet in frequencyMap[t][\"list\"]:\n",
    "            if ( tweet[\"id_str\"] == tweetId ):\n",
    "                thisTweet = tweet\n",
    "                break\n",
    "                \n",
    "            if ( \"retweeted_status\" in tweet and tweet[\"retweeted_status\"][\"id_str\"] == tweetId ):\n",
    "                thisTweet = tweet[\"retweeted_status\"]\n",
    "                break\n",
    "                \n",
    "        if ( thisTweet is not None ):\n",
    "            break\n",
    "    \n",
    "    print (\"\\t\", tweetId, tweetRetweetCountMap[tweetId], thisTweet[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retweets seem to be dominated by recent elements. \n",
    "To correct for this, we should remove retweets that are older than the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Top Ten RECENT Retweets:\")\n",
    "\n",
    "foundTweets = 0\n",
    "for tweetId in sortedRetweets:\n",
    "    thisTweet = None\n",
    "    \n",
    "    # Find the most recent copy of the tweet\n",
    "    for t in reversed(sortedTimes):\n",
    "        for tweet in frequencyMap[t][\"list\"]:\n",
    "            if ( tweet[\"id_str\"] == tweetId ):\n",
    "                thisTweet = tweet\n",
    "                break\n",
    "                \n",
    "            if ( \"retweeted_status\" in tweet and tweet[\"retweeted_status\"][\"id_str\"] == tweetId ):\n",
    "                thisTweet = tweet[\"retweeted_status\"]\n",
    "                break\n",
    "                \n",
    "        if ( thisTweet is not None ):\n",
    "            break\n",
    "    \n",
    "    createdTime = datetime.datetime.strptime(thisTweet['created_at'], timeFormat)\n",
    "    \n",
    "    # If tweet creation time is before the crisis, assume irrelevant\n",
    "    if ( createdTime < crisisTime ):\n",
    "        continue\n",
    "        \n",
    "    print (\"\\t\", tweetId, tweetRetweetCountMap[tweetId], thisTweet[\"text\"])\n",
    "    \n",
    "    foundTweets += 1\n",
    "    \n",
    "    if ( foundTweets > 10 ):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Detection w/ Keyword Frequency\n",
    "\n",
    "Twitter is good for breaking news. When an impactful event occurs, we often see a spike on Twitter of the usage of a related keyword. Some examples are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What keywords are we interested in?\n",
    "targetKeywords = crisisInfo[selectedCrisis][\"keywords\"]\n",
    "\n",
    "# Build an empty map for each keyword we are seaching for\n",
    "targetCounts = {x:[] for x in targetKeywords}\n",
    "totalCount = []\n",
    "\n",
    "# For each minute, pull the tweet text and search for the keywords we want\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    # Temporary counter for this minute\n",
    "    localTargetCounts = {x:0 for x in targetKeywords}\n",
    "    localTotalCount = 0\n",
    "    \n",
    "    for tweetObj in timeObj[\"list\"]:\n",
    "        tweetString = tweetObj[\"text\"].lower()\n",
    "\n",
    "        localTotalCount += 1\n",
    "        \n",
    "        # Add to the counter if the target keyword is in this tweet\n",
    "        for keyword in targetKeywords:\n",
    "            if ( keyword in tweetString ):\n",
    "                localTargetCounts[keyword] += 1\n",
    "                \n",
    "    # Add the counts for this minute to the main counter\n",
    "    totalCount.append(localTotalCount)\n",
    "    for keyword in targetKeywords:\n",
    "        targetCounts[keyword].append(localTargetCounts[keyword])\n",
    "        \n",
    "# Now plot the total frequency and frequency of each keyword\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11, 8.5)\n",
    "\n",
    "plt.title(\"Tweet Frequency\")\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "ax.semilogy(range(len(frequencyMap)), totalCount, label=\"Total\")\n",
    "\n",
    "ax.scatter([crisisXCoord], [100], c=\"r\", marker=\"x\", s=100, label=\"Crisis\")\n",
    "\n",
    "for keyword in targetKeywords:\n",
    "    ax.semilogy(range(len(frequencyMap)), targetCounts[keyword], label=keyword)\n",
    "ax.legend()\n",
    "ax.grid(b=True, which=u'major')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<img src=\"files/intermission.jpg\">\n",
    "\n",
    "## Time for a break!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Topic 5: Geographic Data\n",
    "\n",
    "Data in social media can be relevant to an event in three ways: __temporally__ relevant, __geographically__ relevant, or __topically__ relevant.\n",
    "So far, we've looked at temporally relevant data, or data that was posted at about the same time as the target event.\n",
    "Now we'll explore geographically relevant data, or data posted near the event.\n",
    "\n",
    "Twitter allows users to share their GPS locations when tweeting, but only about 2% of tweets have this information. \n",
    "We can extract this geospatial data to look at patterns in different locations. \n",
    "\n",
    "- General plotting\n",
    "- Filtering by a bounding box\n",
    "- Images from target location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting GPS Data\n",
    "\n",
    "Each tweet has a field called \"coordinates\" describing from where the tweet was posted. \n",
    "The field might be null if the tweet contains no location data, or it could contain bounding box information, place information, or GPS coordinates in the form of (longitude, latitude). \n",
    "We want tweets with this GPS data.\n",
    "\n",
    "For more information on tweet JSON formats, check out https://dev.twitter.com/overview/api/tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A frequency map for timestamps to geo-coded tweets\n",
    "geoFrequencyMap = {}\n",
    "geoCount = 0\n",
    "\n",
    "# Save only those tweets with tweet['coordinate']['coordinate'] entity\n",
    "for t in sortedTimes:\n",
    "    geos = list(filter(lambda tweet: tweet[\"coordinates\"] != None and \n",
    "                       \"coordinates\" in tweet[\"coordinates\"], \n",
    "                       frequencyMap[t][\"list\"]))\n",
    "    geoCount += len(geos)\n",
    "    \n",
    "    # Add to the timestamp map\n",
    "    geoFrequencyMap[t] = {\"count\": len(geos), \"list\": geos}\n",
    "\n",
    "print (\"Number of Geo Tweets:\", geoCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPS Frequency\n",
    "\n",
    "What is the frequency of GPS-coded tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11, 8.5)\n",
    "\n",
    "plt.title(\"Geo Tweet Frequency\")\n",
    "\n",
    "gpsFreqList = [geoFrequencyMap[x][\"count\"] for x in sortedTimes]\n",
    "postFreqList = [frequencyMap[x][\"count\"] for x in sortedTimes]\n",
    "\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "xData = range(len(geoFrequencyMap))\n",
    "gpsYData = [x if x > 0 else 0 for x in gpsFreqList]\n",
    "freqYData = [x if x > 0 else 0 for x in postFreqList]\n",
    "\n",
    "ax.semilogy(xData, freqYData, color=\"blue\", label=\"Posts\")\n",
    "ax.semilogy(xData, gpsYData, color=\"green\", label=\"GPS Posts\")\n",
    "ax.scatter([crisisXCoord], [100], c=\"r\", marker=\"x\", s=100, label=\"Crisis\")\n",
    "\n",
    "ax.grid(b=True, which=u'major')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting GPS Data\n",
    "\n",
    "Now that we have a list of all the tweets with GPS coordinates, we can plot from where in the world these tweets were posted. \n",
    "To make this plot, we can leverage the Basemap package to make a map of the world and convert GPS coordinates to *(x, y)* coordinates we can then plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import functools\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Create a list of all geo-coded tweets\n",
    "tmpGeoList = [geoFrequencyMap[t][\"list\"] for t in sortedTimes]\n",
    "geoTweets = functools.reduce(lambda x, y: x + y, tmpGeoList)\n",
    "\n",
    "# For each geo-coded tweet, extract its GPS coordinates\n",
    "geoCoord = [x[\"coordinates\"][\"coordinates\"] for x in geoTweets]\n",
    "\n",
    "# Now we build a map of the world using Basemap\n",
    "land_color = 'lightgray'\n",
    "water_color = 'lightblue'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24,24))\n",
    "worldMap = Basemap(projection='merc', llcrnrlat=-80, urcrnrlat=80,\n",
    "                   llcrnrlon=-180, urcrnrlon=180, resolution='l')\n",
    "\n",
    "worldMap.fillcontinents(color=land_color, lake_color=water_color, zorder=1)\n",
    "worldMap.drawcoastlines()\n",
    "worldMap.drawparallels(np.arange(-90.,120.,30.))\n",
    "worldMap.drawmeridians(np.arange(0.,420.,60.))\n",
    "worldMap.drawmapboundary(fill_color=water_color, zorder=0)\n",
    "ax.set_title('World Tweets')\n",
    "\n",
    "# Convert points from GPS coordinates to (x,y) coordinates\n",
    "convPoints = [worldMap(p[0], p[1]) for p in geoCoord]\n",
    "x = [p[0] for p in convPoints]\n",
    "y = [p[1] for p in convPoints]\n",
    "worldMap.scatter(x, y, s=100, marker='x', color=\"red\", zorder=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering By Location\n",
    "\n",
    "We can use existing Geographic Information System (GIS) tools to determine from where a tweet was posted.\n",
    "For example, we could ask whether a particular tweetÂ was posted from the United States. \n",
    "This filtering is often performed using shape files.\n",
    "For our purposes though, we established a bounding box along with the crisis data, so we'll use that as our filter for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the bounding box for our crisis\n",
    "bBox = crisisInfo[selectedCrisis][\"box\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11,8.5))\n",
    "\n",
    "# Create a new map to hold the shape file data\n",
    "targetMap = Basemap(llcrnrlon=bBox[\"lowerLeftLon\"], \n",
    "                    llcrnrlat=bBox[\"lowerLeftLat\"], \n",
    "                    urcrnrlon=bBox[\"upperRightLon\"], \n",
    "                    urcrnrlat=bBox[\"upperRightLat\"], \n",
    "                    projection='merc',\n",
    "                    resolution='i', area_thresh=10000)\n",
    "\n",
    "targetMap.fillcontinents(color=land_color, lake_color=water_color, \n",
    "                         zorder=1)\n",
    "targetMap.drawcoastlines()\n",
    "targetMap.drawparallels(np.arange(-90.,120.,30.))\n",
    "targetMap.drawmeridians(np.arange(0.,420.,60.))\n",
    "targetMap.drawmapboundary(fill_color=water_color, zorder=0)\n",
    "targetMap.drawcountries()\n",
    "\n",
    "# Now we build the polygon for filtering\n",
    "# Convert from lon, lat of lower-left to x,y coordinates\n",
    "llcCoord = targetMap(bBox[\"lowerLeftLon\"], bBox[\"lowerLeftLat\"])\n",
    "\n",
    "# Same for upper-right corner\n",
    "urcCoord = targetMap(bBox[\"upperRightLon\"], bBox[\"upperRightLat\"])\n",
    "\n",
    "# Now make the polygon we'll us for filtering\n",
    "boxPoints = np.array([[llcCoord[0], llcCoord[1]], \n",
    "                      [llcCoord[0], urcCoord[1]], \n",
    "                      [urcCoord[0], urcCoord[1]], \n",
    "                      [urcCoord[0], llcCoord[1]]])\n",
    "boundingBox = matplotlib.patches.Polygon(boxPoints)\n",
    "\n",
    "# Maps of timestamps to tweets for inside/outside Ferguson\n",
    "inTargetFreqMap = {}\n",
    "plottablePointsX = []\n",
    "plottablePointsY = []\n",
    "\n",
    "# For each geo-coded tweet, extract coordinates and convert \n",
    "# them to the Basemap space\n",
    "for t in sortedTimes:\n",
    "    geos = geoFrequencyMap[t][\"list\"]\n",
    "    convPoints = [(targetMap(tw[\"coordinates\"][\"coordinates\"][0], tw[\"coordinates\"][\"coordinates\"][1]), tw) for tw in geos]\n",
    "\n",
    "    # Local counters for this time\n",
    "    inTargetFreqMap[t] = {\"count\": 0, \"list\": []}\n",
    "    \n",
    "    # For each point, check if it is within the bounding box or not\n",
    "    for point in convPoints:\n",
    "        x = point[0][0]\n",
    "        y = point[0][1]\n",
    "\n",
    "        if ( boundingBox.contains_point((x, y))):\n",
    "            inTargetFreqMap[t][\"list\"].append(point[1])\n",
    "            plottablePointsX.append(x)\n",
    "            plottablePointsY.append(y)\n",
    "\n",
    "# Plot points in our target\n",
    "targetMap.scatter(plottablePointsX, plottablePointsY, s=100, marker='x', color=\"red\", zorder=2)\n",
    "            \n",
    "# Count the number of tweets that fall in the area\n",
    "targetTweetCount = np.sum([len(inTargetFreqMap[t][\"list\"]) for t in sortedTimes])\n",
    "            \n",
    "print (\"Tweets in Target Area:\", targetTweetCount)\n",
    "print (\"Tweets outside:\", (geoCount - targetTweetCount))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographically Relevant Tweet Content\n",
    "\n",
    "Now that we have a list of tweets from the target area, what are they saying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge our list of relevant tweets\n",
    "geoRelevantTweets = [tw for x in sortedTimes for tw in inTargetFreqMap[x][\"list\"]]\n",
    "\n",
    "print(\"Time of Crisis:\", crisisTime)\n",
    "\n",
    "# Print the first few tweets\n",
    "for tweet in geoRelevantTweets[:10]:\n",
    "    print(\"Tweet By:\", tweet[\"user\"][\"screen_name\"])\n",
    "    print(\"\\t\", \"Tweet Text:\", tweet[\"text\"])\n",
    "    print(\"\\t\", \"Tweet Time:\", tweet[\"created_at\"])\n",
    "    print(\"\\t\", \"Source:\", tweet[\"source\"])\n",
    "    print(\"\\t\", \"Retweets:\", tweet[\"retweet_count\"])\n",
    "    print(\"\\t\", \"Favorited:\", tweet[\"favorite_count\"])\n",
    "    print(\"\\t\", \"Twitter's Guessed Language:\", tweet[\"lang\"])\n",
    "    if ( \"place\" in tweet ):\n",
    "        print(\"\\t\", \"Tweet Location:\", tweet[\"place\"][\"full_name\"])\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media from Within Target\n",
    "\n",
    "With this filtered list of tweets, we can extract media posted from the evnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "\n",
    "geoTweetsWithMedia = list(filter(lambda tweet: \"media\" in tweet[\"entities\"], geoRelevantTweets))\n",
    "print (\"Tweets with Media:\", len(geoTweetsWithMedia))\n",
    "\n",
    "if ( len(geoTweetsWithMedia) == 0 ):\n",
    "    print (\"Sorry, not tweets with media...\")\n",
    "\n",
    "for tweet in geoTweetsWithMedia:\n",
    "    imgUrl = tweet[\"entities\"][\"media\"][0][\"media_url\"]\n",
    "    print (tweet[\"text\"])\n",
    "    display(Image(url=imgUrl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Topic 6: Content and Sentiment Analysis\n",
    "\n",
    "Another popular type of analysis people do on social networks is \"sentiment analysis,\" which is used to figure out how people **feel** about a specific topic.\n",
    "Some tools also provide measurements like subjectivity/objectivity of text content.\n",
    "\n",
    "We'll cover:\n",
    "\n",
    "- Topically Relevant Filtering\n",
    "- Sentiment, Subjectivity, and Objectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topically Relevant Tweets\n",
    "\n",
    "Before we filter for sentiment and such, we've seen that Twitter has a lot of noise and irrelevant data.\n",
    "We should clean this data a bit before this analysis.\n",
    "To do so, we'll filter our data so that it only contains tweets with relevant keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What keywords are we interested in?\n",
    "targetKeywords = crisisInfo[selectedCrisis][\"keywords\"]\n",
    "\n",
    "# Map for storing topically relevant data\n",
    "topicRelevantMap = {}\n",
    "\n",
    "# For each minute, pull the tweet text and search for the keywords we want\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    topicRelevantMap[t] = {\"count\": 0, \"list\": []}\n",
    "    \n",
    "    for tweetObj in timeObj[\"list\"]:\n",
    "        tweetString = tweetObj[\"text\"].lower()\n",
    "\n",
    "        # Add to the counter if the target keyword is in this tweet\n",
    "        for keyword in targetKeywords:\n",
    "            if ( keyword.lower() in tweetString ):\n",
    "                topicRelevantMap[t][\"list\"].append(tweetObj)\n",
    "                topicRelevantMap[t][\"count\"] += 1\n",
    "                \n",
    "                break\n",
    "\n",
    "        \n",
    "# Now plot the total frequency and frequency of each keyword\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11, 8.5)\n",
    "\n",
    "plt.title(\"Tweet Frequency\")\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "ax.semilogy(range(len(frequencyMap)), totalCount, label=\"Total\")\n",
    "\n",
    "ax.scatter([crisisXCoord], [100], c=\"r\", marker=\"x\", s=100, label=\"Crisis\")\n",
    "\n",
    "relYData = [topicRelevantMap[t][\"count\"] for t in sortedTimes]\n",
    "ax.semilogy(range(len(relYData)), relYData, label=\"Relevant\")\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(b=True, which=u'major')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highly Important Relevant Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allTweets = [x for t in sortedTimes for x in topicRelevantMap[t][\"list\"]]\n",
    "\n",
    "# get the top retweeted tweets\n",
    "onlyRetweets = filter(lambda x: \"retweeted_status\" in x, allTweets)\n",
    "topTweets = sorted(onlyRetweets, key=lambda x: x[\"retweeted_status\"][\"retweet_count\"], \n",
    "                   reverse=True)[:10]\n",
    "\n",
    "print(\"Top Retweets:\")\n",
    "for x in topTweets:\n",
    "    print(x[\"id\"], x[\"user\"][\"screen_name\"], x[\"retweeted_status\"][\"retweet_count\"], x[\"text\"])\n",
    "\n",
    "# get tweets from users with the msot followers\n",
    "topTweets = sorted(allTweets, key=lambda x: x[\"user\"][\"followers_count\"], reverse=True)[:10]\n",
    "\n",
    "print()\n",
    "print(\"Top Accounts:\")\n",
    "for x in topTweets:\n",
    "    print(x[\"id\"], x[\"user\"][\"screen_name\"], x[\"user\"][\"followers_count\"], x[\"text\"])\n",
    "    \n",
    "    \n",
    "# get the top retweeted tweets but only from verified accounts\n",
    "verifiedTweets = filter(lambda x: x[\"retweeted_status\"][\"user\"][\"verified\"], onlyRetweets)\n",
    "topTweets = sorted(verifiedTweets, key=lambda x: x[\"retweeted_status\"][\"retweet_count\"], \n",
    "                   reverse=True)[:10]\n",
    "\n",
    "print()\n",
    "print(\"Top Retweets from Verified Accounts:\")\n",
    "for x in verifiedTweets:\n",
    "    print(x[\"id\"], x[\"user\"][\"screen_name\"], x[\"retweet_count\"], x[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Geo-data Comparison\n",
    "\n",
    "An interesting comparison might be to look at the areas of concentration of relevant tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A frequency map for timestamps to geo-coded tweets\n",
    "relGeoFreqMap = {}\n",
    "relGeoCount = 0\n",
    "\n",
    "# Save only those tweets with tweet['coordinate']['coordinate'] entity\n",
    "for t in sortedTimes:\n",
    "    geos = list(filter(lambda tweet: tweet[\"coordinates\"] != None and \n",
    "                       \"coordinates\" in tweet[\"coordinates\"], \n",
    "                       topicRelevantMap[t][\"list\"]))\n",
    "    relGeoCount += len(geos)\n",
    "    \n",
    "    # Add to the timestamp map\n",
    "    relGeoFreqMap[t] = {\"count\": len(geos), \"list\": geos}\n",
    "\n",
    "print (\"Number of Relevant Geo Tweets:\", relGeoCount)\n",
    "\n",
    "# Create a list of all geo-coded tweets\n",
    "tmpGeoList = [relGeoFreqMap[t][\"list\"] for t in sortedTimes]\n",
    "relGeoTweets = functools.reduce(lambda x, y: x + y, tmpGeoList)\n",
    "\n",
    "# For each geo-coded tweet, extract its GPS coordinates\n",
    "relGeoCoord = [x[\"coordinates\"][\"coordinates\"] for x in relGeoTweets]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24,24))\n",
    "worldMap = Basemap(projection='merc', llcrnrlat=-80, urcrnrlat=80,\n",
    "                   llcrnrlon=-180, urcrnrlon=180, resolution='l')\n",
    "\n",
    "worldMap.fillcontinents(color=land_color, lake_color=water_color, zorder=1)\n",
    "worldMap.drawcoastlines()\n",
    "worldMap.drawparallels(np.arange(-90.,120.,30.))\n",
    "worldMap.drawmeridians(np.arange(0.,420.,60.))\n",
    "worldMap.drawmapboundary(fill_color=water_color, zorder=0)\n",
    "worldMap.drawcountries()\n",
    "ax.set_title('Global Relevant Tweets')\n",
    "\n",
    "# Convert points from GPS coordinates to (x,y) coordinates\n",
    "allConvPoints = [worldMap(p[0], p[1]) for p in geoCoord]\n",
    "x = [p[0] for p in allConvPoints]\n",
    "y = [p[1] for p in allConvPoints]\n",
    "worldMap.scatter(x, y, s=100, marker='x', color=\"blue\", zorder=2)\n",
    "\n",
    "# Convert points from GPS coordinates to (x,y) coordinates\n",
    "relConvPoints = [worldMap(p[0], p[1]) for p in relGeoCoord]\n",
    "x = [p[0] for p in relConvPoints]\n",
    "y = [p[1] for p in relConvPoints]\n",
    "worldMap.scatter(x, y, s=100, marker='x', color=\"red\", zorder=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation:__ Most topically relevant tweets are not geotagged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis w/ TextBlob\n",
    "\n",
    "TextBlob is a nice Python package that provides a number of useful text processing capabilities.\n",
    "We will use it for sentiment analysis to calculate polarity and subjectivity for each relevant tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Sentiment values\n",
    "polarVals = []\n",
    "objVals = []\n",
    "\n",
    "# For each minute, pull the tweet text and search for the keywords we want\n",
    "for t in sortedTimes:\n",
    "    timeObj = topicRelevantMap[t]\n",
    "    \n",
    "    # For calculating averages\n",
    "    localPolarVals = []\n",
    "    localObjVals = []\n",
    "    \n",
    "    for tweetObj in timeObj[\"list\"]:\n",
    "        tweetString = tweetObj[\"text\"].lower()\n",
    "\n",
    "        blob = TextBlob(tweetString)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        objectivity = blob.sentiment.subjectivity\n",
    "        \n",
    "        localPolarVals.append(polarity)\n",
    "        localObjVals.append(objectivity)\n",
    "        \n",
    "    # Add data to the polarity and objectivity measure arrays\n",
    "    if ( len(timeObj[\"list\"]) > 10 ):\n",
    "        polarVals.append(np.mean(localPolarVals))\n",
    "        objVals.append(np.mean(localObjVals))\n",
    "    else:\n",
    "        polarVals.append(0.0)\n",
    "        objVals.append(0.0)\n",
    "\n",
    "        \n",
    "# Now plot this sentiment data\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11, 8.5)\n",
    "\n",
    "plt.title(\"Sentiment\")\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "xData = range(len(sortedTimes))\n",
    "\n",
    "ax.scatter([crisisXCoord], [0], c=\"r\", marker=\"x\", s=100, label=\"Crisis\")\n",
    "\n",
    "# Polarity is scaled [-1, 1], for negative and positive polarity\n",
    "ax.plot(xData, polarVals, label=\"Polarity\")\n",
    "\n",
    "# Subjetivity is scaled [0, 1], with 0 = objective, 1 = subjective\n",
    "ax.plot(xData, objVals, label=\"Subjectivity\")\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(b=True, which=u'major')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")\n",
    "import nltk.sentiment.util\n",
    "import nltk.sentiment.vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader = nltk.sentiment.vader.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sentiment values\n",
    "polarVals = []\n",
    "\n",
    "# For each minute, pull the tweet text and search for the keywords we want\n",
    "for t in sortedTimes:\n",
    "    timeObj = topicRelevantMap[t]\n",
    "    \n",
    "    # For calculating averages\n",
    "    localPolarVals = []\n",
    "    \n",
    "    for tweetObj in timeObj[\"list\"]:\n",
    "        tweetString = tweetObj[\"text\"].lower()\n",
    "\n",
    "        polarity = vader.polarity_scores(tweetString)[\"compound\"]\n",
    "        \n",
    "        localPolarVals.append(polarity)\n",
    "        \n",
    "    # Add data to the polarity and objectivity measure arrays\n",
    "    if ( len(timeObj[\"list\"]) > 10 ):\n",
    "        polarVals.append(np.mean(localPolarVals))\n",
    "    else:\n",
    "        polarVals.append(0.0)\n",
    "\n",
    "        \n",
    "# Now plot this sentiment data\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11, 8.5)\n",
    "\n",
    "plt.title(\"Sentiment\")\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "xData = range(len(sortedTimes))\n",
    "\n",
    "ax.scatter([crisisXCoord], [0], c=\"r\", marker=\"x\", s=100, label=\"Crisis\")\n",
    "\n",
    "# Polarity is scaled [-1, 1], for negative and positive polarity\n",
    "ax.plot(xData, polarVals, label=\"Polarity\")\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(b=True, which=u'major')\n",
    "\n",
    "plt.ylim((-0.3, 0.55))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Topic 7: Topic Modeling\n",
    "\n",
    "Along with sentiment analysis, a question often asked of social networks is \"What are people talking about?\" \n",
    "We can answer this question using tools from topic modeling and natural language processing.\n",
    "With crises, people can have many responses, from sharing specific data about the event, sharing condolonces, or opening their homes to those in need.\n",
    "\n",
    "To generate these topic models, we will use the Gensim package's implementation of Latent Dirichlet Allocation (LDA), which basically constructs a set of topics where each topic is described as a probability distribution over the words in our tweets. \n",
    "Several other methods for topic modeling exist as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gotta pull in a bunch of packages for this\n",
    "import gensim.models.ldamulticore\n",
    "import gensim.matutils\n",
    "import sklearn.cluster\n",
    "import sklearn.feature_extraction \n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract all relevant tweets' text for building our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all tweets and conver to lowercase\n",
    "allTweetText = [x[\"text\"].lower() for t in sortedTimes for x in topicRelevantMap[t][\"list\"]]\n",
    "\n",
    "print (\"All Tweet Count:\", len(allTweetText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a list of stop words (words we don't care about) and build a feature generator (the vectorizer) that assigns integer keys to tokens and counts the number of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enStop = stopwords.words('english')\n",
    "esStop = stopwords.words('spanish')\n",
    "\n",
    "# Skip stop words, retweet signs, @ symbols, and URL headers\n",
    "stopList = enStop + esStop + [\"http\", \"https\", \"rt\", \"@\", \":\", \"co\"]\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(strip_accents='unicode', \n",
    "                                                             tokenizer=None,\n",
    "                                                             token_pattern='(?u)#?\\\\b\\\\w+[\\'-]?\\\\w+\\\\b',\n",
    "                                                             stop_words=stopList)\n",
    "\n",
    "# Analyzer\n",
    "analyze = vectorizer.build_analyzer() \n",
    "\n",
    "# Create a vectorizer for all our content\n",
    "vectorizer.fit(allTweetText)\n",
    "\n",
    "# Get all the words in our text\n",
    "names = vectorizer.get_feature_names()\n",
    "\n",
    "# Create a map for vectorizer IDs to words\n",
    "id2WordDict = dict(zip(range(len(vectorizer.get_feature_names())), names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the vectorizer to transform our tweet text into a feature set, which essentially is a table with rows of tweets, columns for each keyword, and each cell is the number of times that keyword appears in that tweet.\n",
    "\n",
    "We then convert that table into a model the Gensim package can handle, apply LDA, and grab the top 10 topics, 10 words that describe that topic, and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a corpus for \n",
    "corpus = vectorizer.transform(allTweetText)\n",
    "gsCorpus = gensim.matutils.Sparse2Corpus(corpus, documents_columns=False)\n",
    "        \n",
    "lda = gensim.models.LdaMulticore(gsCorpus, \n",
    "                                 id2word=id2WordDict,\n",
    "                                 num_topics=20, \n",
    "                                 passes=2) # ++ passes for better results\n",
    "\n",
    "ldaTopics = lda.show_topics(num_topics=10, \n",
    "                            num_words=10, \n",
    "                            formatted=False)\n",
    "\n",
    "for (i, tokenList) in ldaTopics:\n",
    "    print (\"Topic %d:\" % i, ' '.join([pair[0] for pair in tokenList]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also be a little more strict and get rid of some noise by looking only at words with more than X characters.\n",
    "Stop words are often short, so by putting a floor on the length of a token, we can theoretically get higher-signal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docArrays = filter(lambda x: len(x) > 4, [y for x in allTweetText for y in analyze(x)])\n",
    "fd = nltk.FreqDist(docArrays)\n",
    "\n",
    "print (\"Most common from analyzer:\")\n",
    "for x in fd.most_common(20):\n",
    "    print (x[0], x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Topic 8: Network Analysis\n",
    "\n",
    "Information flows and social networks are important considerations during crises, when people are trying to get updates on safe spaces, loved ones, places of shelter, etc.\n",
    "Twitter is noisy though, and a lot of the data may be irrelevant, condolences/thoughts expressed by celebrities, or otherwise uninformative.\n",
    "Using network analysis, we can get some idea about who the most important Twitter users were during this time, and how people split into groups online.\n",
    "\n",
    "For this analysis, we'll use the NetworkX package to construct a social graph of how people interact. Each person in our Twitter data will be a node in our graph, and edges in the graph will represent mentions during this timeframe.\n",
    "Then we will explore a few simple analytical methods in network analysis, including:\n",
    "\n",
    "- Central accounts\n",
    "- Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Building\n",
    "\n",
    "To limit the amount of data we're looking at, we'll only build the network for people who tweeted about a relevant keyword and the people they mention. \n",
    "We build this network simply by iterating through all the tweets in our relevant list and extract the \"user_mentions\" list from the \"entities\" section of the tweet object.\n",
    "For each mention a user makes, we will add an edge from that user to the user he/she mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# We'll use a directed graph since mentions/retweets are directional\n",
    "graph = nx.DiGraph()\n",
    "    \n",
    "for tweet in [x for t in sortedTimes for x in topicRelevantMap[t][\"list\"]]:\n",
    "    userName = tweet[\"user\"][\"screen_name\"]\n",
    "    graph.add_node(userName)\n",
    "\n",
    "    mentionList = tweet[\"entities\"][\"user_mentions\"]\n",
    "\n",
    "    for otherUser in mentionList:\n",
    "        otherUserName = otherUser[\"screen_name\"]\n",
    "        if ( graph.has_node(otherUserName) == False ):\n",
    "            graph.add_node(otherUserName)\n",
    "        graph.add_edge(userName, otherUserName)\n",
    "        \n",
    "print (\"Number of Users:\", len(graph.node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Users\n",
    "\n",
    "In network analysis, \"centrality\" is used to measure the importance of a given node. \n",
    "Many different types of centrality are used to describe various types of importance though.\n",
    "Examples include \"closeness centrality,\" which measures how close a node is to all other nodes in the network, versus \"betweeness centrality,\" which measures how many shortest paths run through the given node.\n",
    "Nodes with high closeness centrality are important for rapidly disseminating information or spreading disease, whereas nodes with high betweeness are more important to ensure the network stays connected.\n",
    "\n",
    "The PageRank is another algorithm for measuring importance and was proposed by Sergey Brin and Larry Page for the early version of Google's search algorithm.\n",
    "NetworkX has an implementation of the PageRank algorithm that we can use to look at the most important/authoritative users on Twitter based on their connections to other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we prune for performance reasons\n",
    "# remove all nodes with few edges\n",
    "\n",
    "nodeList = [n for n,d in graph.degree_iter() if d<2]\n",
    "graph.remove_nodes_from(nodeList)\n",
    "print (\"Number of Remaining Users:\", len(graph.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THis may take a while\n",
    "pageRankList = nx.pagerank_numpy(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "highRankNodes = sorted(pageRankList.keys(), key=pageRankList.get, reverse=True)\n",
    "for x in highRankNodes[:20]:\n",
    "    print (x, pageRankList[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "pos = nx.spring_layout(graph, scale=100, iterations=100, k=0.2)\n",
    "nx.draw(graph, \n",
    "        pos, \n",
    "        node_color='#A0CBE2', \n",
    "        width=1, \n",
    "        with_labels=False,\n",
    "        node_size=50)\n",
    "\n",
    "hrNames = highRankNodes[:20]\n",
    "hrDict = dict(zip(hrNames, hrNames))\n",
    "hrValues = [pageRankList[x] for x in hrNames]\n",
    "\n",
    "nx.draw_networkx_nodes(graph,pos,nodelist=hrNames,\n",
    "                       node_size=200,\n",
    "                       node_color=hrValues,\n",
    "                       cmap=plt.cm.Reds_r)\n",
    "\n",
    "nx.draw_networkx_labels(graph,\n",
    "                        pos,\n",
    "                        labels=hrDict,\n",
    "                        fontsize=36,\n",
    "                        font_color=\"g\")\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
